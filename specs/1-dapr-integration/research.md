# Research: Dapr Infrastructure Integration

**Feature**: Dapr Infrastructure Integration (1-dapr-integration)
**Date**: 2026-02-08
**Phase**: 0 (Research & Discovery)

## Overview

This document consolidates research findings for integrating Dapr into the existing Kubernetes-deployed Todo Application. All technical decisions, patterns, and prerequisites are documented here to guide implementation.

---

## Research Task 1: Dapr Installation and Prerequisites

### Decision
Dapr runtime version 1.10+ must be installed on Minikube cluster before Helm deployment.

### Rationale
- Dapr 1.10+ includes stable APIs for state management, pub/sub, and service invocation
- Dapr control plane components (operator, sidecar-injector, placement, sentry) must be running for sidecar injection to work
- Minikube supports Dapr installation via `dapr init -k` command

### Alternatives Considered
- **Alternative 1**: Include Dapr installation in Helm chart
  - **Rejected**: Dapr is cluster-level infrastructure, should be installed separately
  - Installing Dapr as part of application Helm chart creates tight coupling

- **Alternative 2**: Assume Dapr is always present
  - **Rejected**: Without verification, deployment could fail silently
  - Better to validate prerequisites explicitly in Stage 1

### Prerequisites Checklist
```bash
# Required components
- [ ] Dapr CLI installed (version 1.10+)
- [ ] Kubernetes cluster accessible (kubectl configured)
- [ ] Dapr runtime initialized: `dapr init -k`
- [ ] Dapr control plane healthy: `dapr status -k`
- [ ] Default namespace available

# Expected control plane components:
- dapr-operator
- dapr-sidecar-injector
- dapr-placement-server
- dapr-sentry (for mTLS)
- dapr-dashboard (optional)
```

### Verification Commands
```bash
# Check Dapr CLI version
dapr version
# Expected: CLI version: 1.10.x, Runtime version: 1.10.x

# Check control plane status
dapr status -k
# Expected: All components Running and Healthy

# Verify sidecar injector
kubectl get deploy dapr-sidecar-injector -n dapr-system
# Expected: READY 1/1
```

---

## Research Task 2: Redis Infrastructure Analysis

### Decision
Redis will be deployed as a Helm subchart dependency, included in `helm/todo-chart/Chart.yaml`.

### Rationale
- Redis needed for both state store and pub/sub components
- Including as subchart ensures Redis is available when Helm chart installs
- Single Redis instance can serve both components (resource efficient)
- Helm manages Redis lifecycle alongside application

### Alternatives Considered
- **Alternative 1**: Assume external Redis
  - **Rejected**: Adds external dependency, complicates local development
  - External Redis configuration varies by environment

- **Alternative 2**: Deploy Redis manually before Helm install
  - **Rejected**: Adds manual step, violates "single Helm install" requirement
  - Users must remember to install Redis separately

- **Alternative 3**: Use embedded Redis in application pods
  - **Rejected**: Not production-ready, adds complexity to application
  - Defeats purpose of distributed architecture

### Redis Helm Subchart Configuration

**Chart.yaml** (add dependency):
```yaml
dependencies:
  - name: redis
    version: "17.x.x"
    repository: https://charts.bitnami.com/bitnami
    condition: redis.enabled
```

**values.yaml** (Redis configuration):
```yaml
redis:
  enabled: true
  auth:
    enabled: true
    password: ""  # Set via --set or values override
  master:
    service:
      type: ClusterIP
      ports:
        redis: 6379
  replica:
    replicaCount: 0  # Single instance for dev (can scale in prod)
```

### Connection String Format
```
redisHost: redis-master.default.svc.cluster.local:6379
# Pattern: {release-name}-redis-master.{namespace}.svc.cluster.local:6379
```

### Kubernetes Secret Structure
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: redis-secret
  namespace: default
type: Opaque
stringData:
  password: <redis-password>  # Provided during Helm install
```

### Alternative: Reference Bitnami Redis Secret
Bitnami Redis chart automatically creates a secret. Dapr components can reference it:
```yaml
metadata:
- name: redisPassword
  secretKeyRef:
    name: todo-app-redis  # Auto-generated by Bitnami chart
    key: redis-password
```

---

## Research Task 3: Existing Helm Chart Analysis

### Current Structure
```
helm/todo-chart/
├── Chart.yaml
├── values.yaml
└── templates/
    ├── _helpers.tpl
    ├── frontend-deployment.yaml
    ├── frontend-service.yaml
    ├── backend-deployment.yaml
    ├── backend-service.yaml
    ├── configmap.yaml
    └── secrets.yaml
```

### Naming Conventions Observed
- Template files: `{service}-{resource-type}.yaml` (e.g., `frontend-deployment.yaml`)
- Helper templates: `_helpers.tpl` prefix
- No existing subdirectories in templates/

### Existing Annotation Patterns
**Current frontend-deployment.yaml** excerpt:
```yaml
spec:
  template:
    metadata:
      labels:
        app: frontend
        # No existing annotations observed
```

**Modification Strategy**: Add annotations conditionally:
```yaml
spec:
  template:
    metadata:
      labels:
        app: frontend
      annotations:
        {{- if .Values.dapr.enabled }}
        dapr.io/enabled: "true"
        dapr.io/app-id: {{ .Values.frontend.dapr.appId | quote }}
        dapr.io/app-port: {{ .Values.frontend.dapr.appPort | quote }}
        {{- end }}
```

### values.yaml Structure Observation
- Top-level keys: `frontend`, `backend`, `database`, `config`, `secrets`, `ai`
- Each service has: `image`, `service`, `replicaCount`, `resources`
- Pattern: Service-specific configuration nested under service name

**Addition Strategy**: Add new top-level `dapr` key:
```yaml
dapr:
  enabled: true
  # Global Dapr settings

frontend:
  # Existing frontend config
  dapr:
    # Frontend-specific Dapr config
```

---

## Research Task 4: Dapr Annotation Best Practices

### Required Annotations (Minimum)
```yaml
dapr.io/enabled: "true"           # Enable Dapr sidecar injection
dapr.io/app-id: "backend-app"     # Unique application identifier
dapr.io/app-port: "8000"          # Port application listens on
```

### Recommended Annotations (Observability)
```yaml
dapr.io/log-level: "info"         # Options: debug, info, warn, error
dapr.io/enable-metrics: "true"    # Enable Prometheus metrics
dapr.io/metrics-port: "9090"      # Metrics endpoint port
```

### Optional Annotations (Advanced)
```yaml
dapr.io/config: "appconfig"       # Dapr Configuration resource name
dapr.io/app-protocol: "http"      # Options: http, https, grpc, grpcs
dapr.io/enable-debug: "false"     # Debug mode
dapr.io/log-as-json: "true"       # JSON-formatted logs
dapr.io/sidecar-cpu-limit: "1.0"  # CPU limit for sidecar
dapr.io/sidecar-memory-limit: "512Mi"  # Memory limit for sidecar
```

### App-ID Naming Convention
**Pattern**: `{service-name}-app` (lowercase, hyphen-separated)

Examples:
- Frontend: `frontend-app`
- Backend: `backend-app`
- Chatbot: `chatbot-app` (if separate service in future)

**Rationale**:
- Descriptive and intuitive
- Consistent with Kubernetes naming conventions
- Easy to identify in `dapr list -k` output

### Default Dapr Ports
- **HTTP Port**: 3500 (non-configurable, Dapr standard)
- **gRPC Port**: 50001 (non-configurable, Dapr standard)
- **Metrics Port**: 9090 (configurable, Prometheus standard)

**Impact**: Application code must call `localhost:3500` for Dapr API, not configurable.

---

## Research Task 5: Dapr Component YAML Structure

### State Store Component (state.redis)

**File**: `helm/todo-chart/templates/dapr-components/statestore.yaml`

```yaml
apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: statestore
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "todo-chart.labels" . | nindent 4 }}
spec:
  type: state.redis
  version: v1
  metadata:
  # Redis connection
  - name: redisHost
    value: {{ .Values.dapr.stateStore.redisHost | quote }}
  - name: redisPassword
    secretKeyRef:
      name: {{ .Values.dapr.stateStore.secretName }}
      key: {{ .Values.dapr.stateStore.secretKey }}

  # Optional settings
  - name: enableTLS
    value: "false"
  - name: maxRetries
    value: "3"
  - name: maxRetryBackoff
    value: "2s"

  # Scoping: Only backend-app can access
  scopes:
  - {{ .Values.backend.dapr.appId }}
```

**Key Metadata Fields**:
- `redisHost`: Connection string (required)
- `redisPassword`: Secret reference (required if auth enabled)
- `enableTLS`: TLS encryption (optional, default false)
- `maxRetries`: Retry attempts for transient failures
- `maxRetryBackoff`: Exponential backoff duration

### Pub/Sub Component (pubsub.redis)

**File**: `helm/todo-chart/templates/dapr-components/pubsub.yaml`

```yaml
apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: pubsub
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "todo-chart.labels" . | nindent 4 }}
spec:
  type: pubsub.redis
  version: v1
  metadata:
  # Redis connection
  - name: redisHost
    value: {{ .Values.dapr.pubsub.redisHost | quote }}
  - name: redisPassword
    secretKeyRef:
      name: {{ .Values.dapr.pubsub.secretName }}
      key: {{ .Values.dapr.pubsub.secretKey }}

  # Pub/Sub settings
  - name: consumerID
    value: "{{ .Release.Name }}-consumer"
  - name: enableTLS
    value: "false"
  - name: redeliverInterval
    value: "30s"
  - name: processingTimeout
    value: "60s"
  - name: maxRetries
    value: "3"

  # Scoping: Only backend-app can publish/subscribe
  scopes:
  - {{ .Values.backend.dapr.appId }}
```

**Key Metadata Fields**:
- `consumerID`: Unique consumer group ID (required)
- `redeliverInterval`: Time before redelivery attempt
- `processingTimeout`: Max time to process message
- `maxRetries`: Retry attempts before DLQ

### Component Scoping
**Purpose**: Restrict which applications can access which components

**Pattern**:
```yaml
scopes:
- backend-app  # Only backend can access this component
```

**Benefits**:
- Security: Prevent unauthorized access
- Resource isolation: Frontend doesn't need state store access
- Clear boundaries: Explicit service responsibilities

---

## Research Task 6: Service Invocation Patterns

### Dapr HTTP Invocation URL Format
```
http://localhost:{dapr-http-port}/v1.0/invoke/{target-app-id}/method/{endpoint-path}
```

**Example**:
```
http://localhost:3500/v1.0/invoke/backend-app/method/tasks
```

**Breakdown**:
- `localhost:3500`: Dapr sidecar (always localhost, port 3500)
- `/v1.0/invoke`: Dapr service invocation API
- `backend-app`: Target app-id (from Dapr annotations)
- `/method/tasks`: Actual endpoint path on target service

### Header Propagation
Dapr automatically forwards HTTP headers:
- `Authorization`: JWT tokens, API keys
- `Content-Type`: Request content type
- Custom headers: `x-correlation-id`, `x-request-id`, etc.

**No code changes needed** for header forwarding.

### Error Handling
Dapr returns standard HTTP status codes:
- `200-299`: Success (forwarded from target service)
- `404`: Target app-id not found
- `500`: Dapr sidecar error
- `503`: Target service unavailable

**Recommendation**: Application should handle 503 gracefully (service temporarily unavailable).

### Performance Overhead
**Expected**: <50ms additional latency compared to direct HTTP

**Factors**:
- Localhost communication (no network overhead)
- Dapr sidecar processing (minimal)
- Added features: retries, circuit breakers, tracing

**Benchmark**: Test in Stage 7 to confirm <50ms overhead.

---

## Research Task 7: Pub/Sub Event Patterns

### CloudEvents 1.0 Specification
**Standard format** for event data:
```json
{
  "specversion": "1.0",
  "type": "com.example.event.type",
  "source": "/source/of/event",
  "id": "unique-event-id",
  "time": "2026-02-08T12:00:00Z",
  "datacontenttype": "application/json",
  "data": {
    "key": "value"
  }
}
```

**Required Fields**:
- `specversion`: CloudEvents version ("1.0")
- `type`: Event type identifier
- `source`: Event source URI
- `id`: Unique event identifier

**Optional Fields**:
- `time`: Event timestamp (ISO 8601)
- `datacontenttype`: MIME type of data
- `data`: Event payload

### Topic Naming Convention
**Pattern**: `{domain}.{entity}.{action}`

**Examples**:
- `task.created` (domain: implicit "todo", entity: task, action: created)
- `task.updated`
- `task.deleted`
- `task.completed`

**Benefits**:
- Hierarchical: Easy to subscribe to patterns (`task.*`)
- Self-documenting: Topic name describes event
- Consistent: Standard format across all events

### Subscription Configuration
**Method 1: Programmatic** (Recommended)

Application exposes `/dapr/subscribe` endpoint:
```json
[
  {
    "pubsubname": "pubsub",
    "topic": "task.created",
    "route": "/events/task-created"
  }
]
```

Dapr polls this endpoint and automatically subscribes.

**Method 2: Declarative** (Kubernetes CRD)
```yaml
apiVersion: dapr.io/v1alpha1
kind: Subscription
metadata:
  name: task-events-subscription
spec:
  pubsubname: pubsub
  topic: task.created
  route: /events/task-created
  scopes:
  - backend-app
```

**Recommendation**: Use programmatic (Method 1) for flexibility.

### Retry Policies and Dead Letter Queue
**Default Retry Policy**:
- Max retries: 3
- Backoff: Exponential (1s, 2s, 4s)
- After max retries: Message moved to DLQ

**DLQ Configuration** (in pubsub component):
```yaml
metadata:
- name: deadLetterTopic
  value: "task-events-dlq"
```

**Handling DLQ**: Separate subscriber can process failed messages for debugging.

---

## Research Task 8: State Store Access Patterns

### Dapr State Store HTTP API

#### Save State (POST)
```bash
POST http://localhost:3500/v1.0/state/{storename}
Content-Type: application/json

[
  {
    "key": "key1",
    "value": {"field": "value"},
    "metadata": {
      "ttlInSeconds": "3600"
    }
  }
]
```

Response: `204 No Content` (success)

#### Get State (GET)
```bash
GET http://localhost:3500/v1.0/state/{storename}/{key}
```

Response: `200 OK` with value, or `204 No Content` (key not found)

#### Delete State (DELETE)
```bash
DELETE http://localhost:3500/v1.0/state/{storename}/{key}
```

Response: `204 No Content` (success)

#### Bulk Get (POST)
```bash
POST http://localhost:3500/v1.0/state/{storename}/bulk
Content-Type: application/json

{
  "keys": ["key1", "key2", "key3"],
  "parallelism": 10
}
```

Response: Array of key-value pairs

### TTL (Time-To-Live) Configuration
**Metadata field**: `ttlInSeconds`

**Example**: 1-hour expiration
```json
"metadata": {
  "ttlInSeconds": "3600"
}
```

**Behavior**:
- State automatically expires after TTL
- GET after expiration returns 204 (not found)
- No manual cleanup needed

### Key Naming Convention
**Pattern**: `{domain}:{type}:{identifier}`

**Examples**:
- `chat:session:123` (chat session for user 123)
- `cache:user:456` (cached user profile for user 456)
- `workflow:task:789` (workflow state for task creation)

**Benefits**:
- Organized: Domain-based grouping
- Readable: Clear purpose from key name
- Queryable: Can list keys by prefix (if supported)

### Consistency and Concurrency
**Strong Consistency** (first-write-wins):
```json
{
  "key": "key1",
  "value": "value1",
  "etag": "previous-etag",
  "options": {
    "concurrency": "first-write",
    "consistency": "strong"
  }
}
```

**Eventual Consistency** (last-write-wins):
```json
{
  "key": "key1",
  "value": "value1",
  "options": {
    "concurrency": "last-write",
    "consistency": "eventual"
  }
}
```

**Recommendation**: Use eventual consistency for chat/cache (performance), strong consistency for critical operations (if needed).

---

## Summary of Research Findings

### Decisions Made
1. ✅ Dapr 1.10+ required, installed separately via `dapr init -k`
2. ✅ Redis deployed as Helm subchart dependency (Bitnami chart)
3. ✅ Dapr components placed in `helm/todo-chart/templates/dapr-components/`
4. ✅ App-ID naming: `{service}-app` pattern
5. ✅ Topic naming: `{domain}.{entity}.{action}` pattern
6. ✅ State key naming: `{domain}:{type}:{identifier}` pattern
7. ✅ Programmatic subscription via `/dapr/subscribe` endpoint
8. ✅ CloudEvents 1.0 standard for all events

### Prerequisites Confirmed
- Dapr CLI and runtime 1.10+
- Kubernetes cluster with kubectl access
- Helm 3+
- Default namespace available

### Components Finalized
- State store: `state.redis` with backend-app scope
- Pub/sub: `pubsub.redis` with backend-app scope
- Redis: Bitnami Helm subchart

### Patterns Established
- Service invocation URL format documented
- Event schema structure defined
- State management API documented
- TTL and key naming conventions set

### Performance Expectations
- Service invocation: <50ms overhead
- Event delivery: <1 second latency
- State operations: <100ms

**Status**: ✅ Research Complete - Ready for Phase 1 Design
